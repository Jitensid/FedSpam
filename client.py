# -*- coding: utf-8 -*-
"""SMS SPAM Federated Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
	https://colab.research.google.com/drive/1u6qsJCwlTQO_CMq0YK-JFbEqBaJDiaQJ

### Environmental Setup
"""

# #@test {"skip": true}

# # tensorflow_federated_nightly also bring in tf_nightly, which
# # can causes a duplicate tensorboard install, leading to errors.
# !pip uninstall --yes tensorboard tb-nightly

# !pip install --quiet --upgrade tensorflow-federated-nightly
# !pip install --quiet --upgrade nest-asyncio
# !pip install --quiet --upgrade tb-nightly  # or tensorboard, but not both

# import nest_asyncio
# nest_asyncio.apply()

# @test {"skip": true}
# !pip install --quiet --upgrade tensorflow-federated-nightly
# !pip install --quiet --upgrade nest-asyncio

# import nest_asyncio
# nest_asyncio.apply()

import numpy as np
import tensorflow as tf
import flwr as fl
import sys

print(tf.__version__)

SEED = int(sys.argv[1])

np.random.seed(SEED)

"""### Load the Dataset"""

import pandas as pd

british_corpus_df = pd.read_csv("../Data/British Corpus Dataset/british_corpus.csv")
british_corpus_df = british_corpus_df.sample(frac=1).reset_index(drop=True)

british_corpus_df.head()

print(f"No of Records ==> {len(british_corpus_df)}")


def convert_df_to_dict(dataset, target_feature):
    features_dataset_df = dataset.copy().drop(target_feature, axis=1)

    features_dataset_dict = {
        name: np.array(value) for name, value in features_dataset_df.items()
    }

    labels_dataset_df = dataset[[target_feature]]

    labels_dataset_dict = {
        name: np.array(value) for name, value in labels_dataset_df.items()
    }

    return (features_dataset_dict, labels_dataset_dict)


british_corpus_features_dict, british_corpus_labels_dict = convert_df_to_dict(
    british_corpus_df, "Spam"
)

british_corpus_features_dict

british_corpus_labels_dict

"""### Create a TF dataset Now"""

british_corpus_tf_dataset = tf.data.Dataset.from_tensor_slices(
    (british_corpus_features_dict["Message"], british_corpus_labels_dict["Spam"])
)

BATCH_SIZE = 8
british_corpus_tf_dataset = british_corpus_tf_dataset.batch(BATCH_SIZE)

# british_corpus_tf_dataset = tf.data.experimental.make_csv_dataset("/content/british_corpus.csv",label_name='Spam',
#                                                                   batch_size=BATCH_SIZE,
#                                                                   shuffle_buffer_size=SHUFFLE_SIZE)

for message, spam in british_corpus_tf_dataset:
    print(message, spam)
    break

for (message, spam) in british_corpus_tf_dataset.take(1):
    print(message, spam)

"""### Train and Test Split the Dataset"""

TOTAL_SAMPLES = len(british_corpus_df)

TOTAL_BATCHES = int(TOTAL_SAMPLES / BATCH_SIZE)

VALIDATION_SPLIT = 0.2

VALIDATION_BATCHES = int(VALIDATION_SPLIT * TOTAL_BATCHES)

train_dataset = british_corpus_tf_dataset.skip(VALIDATION_BATCHES)
test_dataset = british_corpus_tf_dataset.take(VALIDATION_BATCHES)

import os

GLOVE_EMBEDDING_PATH = "../Glove Embedding/glove.6B.200d.txt"

path_to_glove_file = os.path.join(
    # os.path.expanduser("~"), ".keras/datasets/glove.6B.100d.txt"
    GLOVE_EMBEDDING_PATH,
)

embeddings_index = {}
with open(path_to_glove_file, encoding="utf8") as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print("Found %s word vectors." % len(embeddings_index))


"""### Create VOCABULARY INDEX"""

complete_dataset = list(british_corpus_tf_dataset.unbatch().as_numpy_iterator())

messages = [sample_message for sample_message, sample_label in complete_dataset]

print(messages)

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization

vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)
vectorizer.adapt(messages)

# get top 10 frequent words
vectorizer.get_vocabulary()[:10]

voc = vectorizer.get_vocabulary()
word_index = dict(zip(voc, range(len(voc))))

test_sentence = "yes i thought so thanks aaa"
vectorizer(test_sentence)

"""### Create Embedding Matrix"""

num_tokens = len(voc) + 2
embedding_dim = 200
hits = 0
misses = 0

# Prepare embedding matrix
embedding_matrix = np.zeros((num_tokens, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in embedding index will be all-zeros.
        # This includes the representation for "padding" and "OOV"
        embedding_matrix[i] = embedding_vector
        hits += 1
    else:
        misses += 1
print("Converted %d words (%d misses)" % (hits, misses))

"""### Set the Embedding Layer """

embedding_layer = tf.keras.layers.Embedding(
    num_tokens,
    embedding_dim,
    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),
    trainable=False,
)

"""## Process the Data for Model Training"""


def transform_text(msg, label):
    return (vectorizer(msg), label)


processed_train_dataset = (
    train_dataset.unbatch().map(transform_text).padded_batch(BATCH_SIZE)
)
processed_test_dataset = (
    test_dataset.unbatch().map(transform_text).padded_batch(BATCH_SIZE)
)

"""## Function to get Snapshot of Data"""


def get_sample_data(dataset, samples_to_view=10):
    for sample in dataset.unbatch():
        if samples_to_view == 0:
            return
        print(sample)
        samples_to_view -= 1


"""## Unprocessed Training Dataset"""

get_sample_data(train_dataset)

"""## Unprocessed Test Dataset"""

get_sample_data(test_dataset)

"""## Processed Training Dataset"""

get_sample_data(processed_train_dataset, samples_to_view=1)

"""## Processed Test Dataset"""

get_sample_data(processed_test_dataset, samples_to_view=1)

for msg, label in processed_train_dataset.take(1):
    print(msg, label)

for msg, label in processed_test_dataset.take(1):
    print(msg, label)

different_size_data = [
    "First Sentence",
    "ssssssssssssssssssssssssssssssssssssssssssssssss",
    "Jiten",
]

vectorized_different_size_data = vectorizer(different_size_data)

vectorized_different_size_data

embedding_layer(vectorized_different_size_data)

"""## Create the Model"""


def create_model():
    int_sequences_input = tf.keras.Input(shape=(None,))
    embedded_sequences = embedding_layer(int_sequences_input)
    x = tf.keras.layers.GlobalMaxPooling1D()(embedded_sequences)
    x = tf.keras.layers.Dense(128, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.5)(x)
    preds = tf.keras.layers.Dense(1, activation="sigmoid")(x)
    model = tf.keras.Model(int_sequences_input, preds)
    return model


model = create_model()
model.summary()

model.compile(loss="binary_crossentropy", optimizer="Adam", metrics=["accuracy"])


class SMSMobileClient(fl.client.NumPyClient):
    def get_parameters(self):
        return model.get_weights()

    def fit(self, parameters, config):
        model.set_weights(parameters)
        model.fit(
            processed_train_dataset,
            epochs=10,
            validation_data=processed_test_dataset,
            batch_size=BATCH_SIZE,
        )
        return (
            model.get_weights(),
            BATCH_SIZE * (TOTAL_BATCHES - VALIDATION_BATCHES),
            {},
        )

    def evaluate(self, parameters, config):
        model.set_weights(parameters)
        loss, accuracy = model.evaluate(processed_test_dataset)
        return loss, BATCH_SIZE * VALIDATION_BATCHES, {"accuracy": accuracy}


fl.client.start_numpy_client("localhost:8080", client=SMSMobileClient())